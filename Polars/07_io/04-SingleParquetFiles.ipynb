{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8093b6a5-2db6-4cde-8cc5-caf21f5cfa6c",
   "metadata": {},
   "source": [
    "# Parquet files\n",
    "By the end of this lecture you will be able to:\n",
    "- read from a Parquet file\n",
    "- use query optimisation to read a subset of the data\n",
    "- get the schema of a Parquet file\n",
    "- set the parallelisation strategy for reading a Parquet file\n",
    "- write a Parquet file with compression\n",
    "- write a Parquet file that is larger than memory\n",
    "\n",
    "## What is a Parquet file?\n",
    "A Parquet file is:\n",
    "- where data is stored in columns rather than rows as in a CSV\n",
    "- where each column has a name and a dtype that matches its name and dtype in a `DataFrame`\n",
    "\n",
    "The Apache Parquet and Apache Arrow projects evolved together as columnar formats where Apache Parquet is the format for the data on disk and Apache Arrow is the format for the data in memory.\n",
    "\n",
    "Compared to CSV a Parquet file:\n",
    "- is much faster to read and write than a CSV file\n",
    "- takes much less space on disk, especially once compression is applied\n",
    "- allows Polars to select which columns to read from the file\n",
    "- allows Polars to select which subsets to read from the file (in lazy mode with a predicate pushdown optimisation)\n",
    "- preserves the dtypes of columns\n",
    "\n",
    "*I use Parquet files whenever possible for my data engineering pipelines* with the exception of small files that I want to open to read manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f72df-7ddd-42d0-b330-9e077cd5b4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc3985-7fe2-4f68-9ba1-3e1274f9cc0f",
   "metadata": {},
   "source": [
    "## Creating a Titanic Parquet file\n",
    "We begin by creating a Parquet file from the Titanic CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3cf4c-6039-43de-af64-97c5a07c0fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_file = \"../data/titanic.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd417d-a431-49d1-9618-3acc8bbd4b90",
   "metadata": {},
   "source": [
    "We create the Parquet Titanic directory in the `data_files/parquet` sub-directory of the `io` sub-directory. We use Python's built-in `pathlib` library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc05e05-4b4a-4924-a1f5-a9f3b4952d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_file_path = Path(\"data_files/parquet/titanic\")\n",
    "# If this directory doesn't exist on your machine then create it\n",
    "if not parquet_file_path.exists():\n",
    "    parquet_file_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca690d-4c94-4d2f-9370-8b4b4a6da113",
   "metadata": {},
   "source": [
    "Now we define the path that we will write the Parquet file to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69aed7b-7a31-4bae-8664-9862ac8f0716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parquet_file = \"data_files/parquet/titanic/titanic.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d2dfc-b56c-4f2c-949c-4838fe2d1b1e",
   "metadata": {},
   "source": [
    "Finally we read the CSV and write to the Parquet path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f58883-e657-42db-af01-c1292c215554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.read_csv(csv_file).write_parquet(parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23cb1ae-2a83-40e0-be75-aae4950b4280",
   "metadata": {},
   "source": [
    "## Reading a Parquet file\n",
    "We read the Parquet file to a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78a2bd-2469-44b6-b77a-42b2944e4b16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pl.read_parquet(parquet_file)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f8e01-fcdd-407d-b662-0aeb6246ca0a",
   "metadata": {},
   "source": [
    "A Parquet file has a *footer* where meta data about the file is stored. Polars can quickly read this footer to get the schema of a Parquet file without having to read any data.\n",
    "\n",
    "In Polars we can use the `read_parquet_schema` function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cce1a-3e53-40bb-8cde-b2cbaf98d4ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.read_parquet_schema(parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e720d4-5c90-4120-a7cf-f67ec2423bc5",
   "metadata": {},
   "source": [
    "We can select a subset of columns to read from a Parquet file with the `columns` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc75dc2-8d42-4d78-bb1f-4f35e54ecff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\n",
    "        parquet_file,\n",
    "        columns=[\"Pclass\",\"Name\"]\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510eefdb-01d2-4b9d-9400-0bc7924084b8",
   "metadata": {},
   "source": [
    "We can also specify the number of rows that we want to read with `n_rows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883628bb-19a0-49de-9423-9bec787051ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\n",
    "        parquet_file,\n",
    "        n_rows=2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d0cbb-8ca4-4160-b51e-ccf582e8db1d",
   "metadata": {},
   "source": [
    "If we are running out of memory when reading a Parquet file we can specify `low_memory = True`. This can help to reduce peak memory usage at the expense of a longer load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8776668-910f-4fbd-a0c9-82ffcd80d54e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\n",
    "        parquet_file,\n",
    "        low_memory=True\n",
    "    )\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8b4f4-5305-4abc-9ddb-88ae04f030d3",
   "metadata": {},
   "source": [
    "Polars reads the Parquet file in multiple threads into different chunks of memory. By default Polars then combines all the chunks into a single chunk in parallel. With the `low_memory=True` argument Polars reduces peak memory usage by not doing this recombination in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39803e28-01e7-401f-a750-f9fdc423eba2",
   "metadata": {},
   "source": [
    "As with CSVs, `low_memory = True` only reduces memory usage marginally. If your query is too big to fit in memory use `streaming` in lazy mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee5c16-2b32-488a-b533-cc94cc8a6e33",
   "metadata": {},
   "source": [
    "## Writing a Parquet file\n",
    "When we write a Parquet file we can specify different compression algorithms. I recommend using the default `zstd` in most cases for a good balance of compressed file size on disk and read time into memory. The `lz4` option is an alternative when faster reading and writing is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1adadc-5e0e-4568-8b67-076729087844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .write_parquet(\n",
    "        parquet_file,\n",
    "        compression=\"zstd\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e902ad7-be57-4782-afeb-f9e13a4bcc18",
   "metadata": {},
   "source": [
    "We can also adjust the degree of compression with `compression_level`. The range of values depends on the compression scheme chosen - see the docstrings for details: https://docs.pola.rs/api/python/stable/reference/api/polars.DataFrame.write_parquet.html#polars.DataFrame.write_parquet\n",
    "## Query optimisations on Parquet files\n",
    "\n",
    "### Projection pushdown for subsets of columns\n",
    "When we work in lazy mode in Polars the query optimiser detects when only a subset of columns must be read automatically - this is the projection pushdown query optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93397f15-b271-41f0-8458-0b22f5fab78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_parquet(parquet_file)\n",
    "    .select(\"Pclass\",\"Name\")\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30c153-c2f8-4478-8108-b2bb78cec2ba",
   "metadata": {},
   "source": [
    "### Predicate pushdown for subsets of rows\n",
    "\n",
    "A Parquet file internally is broken into groups of rows (called row groups). Parquet files can store simple min/max statistics of the data in each row group. In a lazy query Polars can use these statistics to determine if only some row groups of the file need to be read.\n",
    "\n",
    "Polars add these statistics by default when it writes a Parquet file. Note that we can write a file with statistics setting `statistics=False` in `write_parquet`. Turning off statistics makes writing the file faster.\n",
    "\n",
    "We make a demonstration here of how the Polars can use the statistics to speed up lazy queries.\n",
    "\n",
    "We are going to create a new `DataFrame` and write it to Parquet. First we make a new directory for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d86dcc-8227-4290-aaf3-c05b2bad3030",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_parquet_file_path = Path(\"data_files/parquet/statistics\")\n",
    "# If this directory doesn't exist on your machine then create it\n",
    "if not statistics_parquet_file_path.exists():\n",
    "    statistics_parquet_file_path.mkdir(parents=True,exist_ok=True)\n",
    "statistics_parquet_file = \"data_files/parquet/statistics/statistics.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b389b88-4cac-430e-b4a0-05264344fcba",
   "metadata": {},
   "source": [
    "We now make a one-column `DataFrame` with many rows and write it to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e034c-f674-4ac3-a6f4-a1a43e2672b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"id\":pl.int_range(0,10_000_000,eager=True)\n",
    "        }\n",
    "    )\n",
    "    .write_parquet(\n",
    "        statistics_parquet_file,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8b01ad-79c1-47d2-9165-0dd91533d56c",
   "metadata": {},
   "source": [
    "If we apply a filter on a **lazy** scan of a Parquet file with statistics, Polars uses the row groups to reduce how much data must be read from the file based on the condition in the `SELECTION` of the query plan.\n",
    "\n",
    "In this example we look for rows where the value of the `id` column is between one million and 2 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e394c9-cee0-40dc-a4e7-a1381d4c97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_parquet(statistics_parquet_file)\n",
    "    .filter(\n",
    "        pl.col(\"id\").is_in([1_000_000,2_000_000])\n",
    "    )\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de30864e-3cbd-4b57-9a37-ee79aef67902",
   "metadata": {},
   "source": [
    "We now compare how long it takes in eager mode to read the whole file and then apply the `filter` to the lazy mode with query optimisations applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ebc63-0140-43f6-a906-b59489990301",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r3\n",
    "(\n",
    "    pl.read_parquet(statistics_parquet_file)\n",
    "    .filter(\n",
    "        pl.col(\"id\") < 1000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff402d0-440b-48ee-a208-21bb3349b7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r3\n",
    "(\n",
    "    pl.scan_parquet(statistics_parquet_file)\n",
    "    .filter(\n",
    "        pl.col(\"id\") < 1000\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eea5fb-da6a-4941-8915-dcace720d065",
   "metadata": {},
   "source": [
    "So in this case the query is much faster as Polars can limit how much data is must read out of the file.\n",
    "\n",
    "### Changing the row group size\n",
    "By default the number of rows in each row group is set to 512^2 (262144). We can adjust this with the `row_group_size` argument. However,...\n",
    "- setting the `row_group_size` to a much smaller value (say 1000) makes the file much bigger\n",
    "- parsing the row group statistics is slow and so even a query that only requires a small number of rows above might be slower with smaller row groups\n",
    "- I've omitted the example but if I try to set the row group size to be smaller the query above is no faster than with the default `row_group_size`\n",
    "\n",
    "Personally, I just keep the default row-group size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7d84c-333a-47c3-b782-712f82af642b",
   "metadata": {},
   "source": [
    "### Taking advantage of row groups\n",
    "To take advantage of row groups you need to:\n",
    "- sort the data in your file so that similar values are clustered together\n",
    "- do lazy queries with a `SELECTION` condition\n",
    "\n",
    "Consider an example where we have a large time series dataset with a datetime column called `time` and a vendor ID column called `VendorID`.\n",
    "\n",
    "If we mainly want to query this by time period then we should sort this by `time` before writing\n",
    "```python\n",
    "df.sort(\"time\",\"VendorID\").write_parquet()\n",
    "```\n",
    "\n",
    "But if we mainly want to query this by vendor then we should sort this by `VendorID` before writing\n",
    "```python\n",
    "df.sort(\"VendorID\",\"time\").write_parquet()\n",
    "```\n",
    "\n",
    "If the rows that meet the `SELECTION` condition are spread through the file then Polars will end up reading many row groups and the query may be slower than on a file without statistics as Polars must evaluate the statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db61508-fcae-4b8c-8a93-0d8624a6bdb2",
   "metadata": {},
   "source": [
    "## Modifying the parallel strategy\n",
    "Polars reads a Parquet file in parallel. It can do this by either reading columns in parallel or row groups in parallel. So how does Polars choose which to do?\n",
    "\n",
    "The basic rule is that Polars counts how many columns and row groups there are and then parallelizes the reading of the larger of the two. So if there are 10 row groups and 20 columns, Polars will read the columns in parallel. This is a solid basic strategy but it isn't guaranteed to be the fastest for your data.\n",
    "\n",
    "We can instead tell Polars to do parallelism by `columns` or `row_groups` by setting the `parallel` argument in the `scan_parquet` function. For example, to read columns in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d7229-670a-4eae-94ee-46a79453ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_parquet(statistics_parquet_file, parallel='columns')\n",
    "    .collect()\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2bfcd-fec0-4e13-a5aa-735638346617",
   "metadata": {},
   "source": [
    "There is a also a new alternative strategy called `prefiltered`, which does a bit of both and can be useful when we are applying a predicate filter. The `prefiltered` strategy first evaluates the pushed-down predicates in parallel and determines which rows need to read. Then, this strategy parallelizes over both the columns and the row groups while filtering out rows that do not need to be read. \n",
    "\n",
    "In some cases with large files and significant filtering the `prefiltered` can provide a significant speedup. In other cases, the `prefiltered` strategy may be slower. You can see for yourself by setting the `parallel` argument in the `scan_parquet` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3aef4-03e8-4bd4-9785-c1a6b2bd76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_parquet(statistics_parquet_file, parallel='prefiltered')\n",
    "    .filter(\n",
    "        pl.col(\"id\") < 1000\n",
    "    )\n",
    "    .collect()\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6735ed8-ae31-4774-a950-8fd32daf17ed",
   "metadata": {},
   "source": [
    "Overall, if querying a large Parquet file is a bottleneck in your pipeline it may be worth experimenting with this argument to see if you can get a speed-up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01929201-39f7-414b-a195-d15215a11d97",
   "metadata": {},
   "source": [
    "### Writing a larger-than-memory Parquet file\n",
    "We can use the streaming engine to process a larger-than-memory query in batches and write the output to a Parquet file in batches. We use the `sink_parquet` method to write to the Parquet file in this way.\n",
    "\n",
    "In this example we create a lazy query by scanning the Parquet file created above and then sink the output to another Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb87b5-cbdc-4ce3-af58-0f06ce6353a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sink_parquet_file = \"data_files/parquet/titanic/titanic_sink.parquet\"\n",
    "(\n",
    "    pl.scan_parquet(parquet_file)\n",
    "    .group_by(\"Pclass\")\n",
    "    .agg(\n",
    "        pl.col(\"PassengerId\").count().alias(\"counts\")\n",
    "    )\n",
    "    .sink_parquet(sink_parquet_file)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6c8ae-00bd-49e4-93d1-07368983e407",
   "metadata": {},
   "source": [
    "The `sink_parquet` approach only requires a `LazyFrame`, the query does not have to begin with a `scan_parquet`. This means we can use it to convert a larger-than-memory CSV file to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6e3fb-65ca-4e6f-9ee3-b72884224d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sink_parquet_file = \"data_files/parquet/titanic/titanic_sink.parquet\"\n",
    "(\n",
    "    pl.scan_csv(csv_file)\n",
    "    .sink_parquet(sink_parquet_file)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5bf43a-0427-41bd-8364-e5de9e47e1ee",
   "metadata": {},
   "source": [
    "There is also a `sink_csv` method available: https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.LazyFrame.sink_csv.html\n",
    "\n",
    "For the `sink_parquet` method to work the full query must work in streaming mode. Recall that we confirm this by confirming that the entire query is within the `--- STREAMING` block in the query plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a91ed6-8fa7-4348-b40d-2b946364ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_csv(csv_file)\n",
    "    .group_by(\"Pclass\")\n",
    "    .agg(\n",
    "        pl.col(\"PassengerId\").count().alias(\"counts\")\n",
    "    )\n",
    "    .explain(streaming=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f109f59-e856-4764-b133-4e054c96e3cd",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "In the exercises you will develop your understanding of:\n",
    "- read and writing Parquet files\n",
    "- categorical dtypes in Parquet files\n",
    "- reading the schema of Parquet files\n",
    "- reading a subset of Parquet files\n",
    "\n",
    "### Exercise 1\n",
    "We will write a new Parquet file for the exercises to this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f894f-3c42-4484-97b6-0c35fc2ace0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_parquet_file = \"data_files/parquet/titanic/titanic_exercise.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cb840-19fd-4dad-97f4-d8256d5f789a",
   "metadata": {},
   "source": [
    "Before we write to this file read the Parquet file created at the start of the notebook to a `DataFrame`. \n",
    "\n",
    "Convert the `Sex` column to `pl.Categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1b6dd-42fe-429a-b508-8100b6875e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.read_parquet(parquet_file)\n",
    "    .with_columns(<blank>)\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641107b-5c72-4b27-9994-dc6b1f4fe9ab",
   "metadata": {},
   "source": [
    "Write the `DataFrame` with a categorical column to `exercise_parquet_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60f32d-bcd0-46cc-9a10-dca657df0b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29c4c7c-9ea1-4282-8387-4c3c6c1bfdb4",
   "metadata": {},
   "source": [
    "Read the schema of `exercise_parquet_file` to confirm whether Parquet can preserve categorical encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb333b54-9d1f-4cdc-8780-e376ad4fb654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c209952-ea58-48b9-b5c9-82fddb489344",
   "metadata": {},
   "source": [
    "Create a lazy query that only reads these columns\n",
    "```python\n",
    "[\"Survived\",\"Pclass\",\"Age\",\"Sex\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ab6ea-c2aa-43a8-b29c-be62b0fe2810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21dfb0e3-4ce8-497a-a8ea-02d2051b92f2",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Solution to exercise 1\n",
    "We will write a new Parquet file for the exercises to this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2355ad-09ca-4463-b110-ee01c3ecccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_parquet_file = \"data_files/parquet/titanic/titanic_exercise.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43db9a-ea2c-44d8-8808-952fa3237c98",
   "metadata": {},
   "source": [
    "Before we write to this file read the Parquet file created at the start of the notebook to a `DataFrame`. \n",
    "\n",
    "Convert the `Sex` column to `pl.Categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c8a61-e682-47f8-9cc8-0b1b29ee5177",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.read_parquet(parquet_file)\n",
    "    .with_columns(pl.col(\"Sex\").cast(pl.Categorical))\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b9157-2688-4764-9d48-597175eec3d3",
   "metadata": {},
   "source": [
    "Write the `DataFrame` with a categorical column to `exercise_parquet_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501bcf8-305c-4bd9-80ca-e4554b43d347",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write_parquet(exercise_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2f63c-f793-480c-8fc4-43f93588c848",
   "metadata": {},
   "source": [
    "Read the schema of `exercise_parquet_file` to confirm whether Parquet can preserve categorical encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490306b-fd6f-4400-aac4-554b2fdafad8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.read_parquet_schema(exercise_parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5df38-7deb-4e8b-ae26-33763f0ec9e9",
   "metadata": {},
   "source": [
    "Create a lazy query that only reads these columns\n",
    "```python\n",
    "[\"Survived\",\"Pclass\",\"Age\",\"Sex\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298bffa-95e6-4b89-891a-dd81af45bbfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_parquet(exercise_parquet_file)\n",
    "    .select([\"Survived\",\"Pclass\",\"Age\",\"Sex\"])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
